{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix PPO :\n",
    "\n",
    "##### Avantages :\n",
    "- PPO est capable de gérer des actions continues\n",
    "- Capable d'apprendre des politiques larges\n",
    "- Plus stable (evitement des oscillations (montrer Policy Gradient))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix CNNPolicy \n",
    "\n",
    "Mieux pour notre probleme car les données d'entrées sont des images\n",
    "\n",
    "L'utilisation d'un reseau convolutionnel + approprié"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix des hyper parametres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **learning_rate**: \n",
    "\n",
    "contrôle la taille des pas dans le processus d'optimisation. Il est important de trouver un taux d'apprentissage approprié qui équilibre la vitesse de convergence et la stabilité.\n",
    "\n",
    "- **gamma**: \n",
    "\n",
    "le facteur de réduction, qui contrôle le poids des récompenses futures dans le processus de décision de l'agent. Un gamma plus faible accorde moins de poids aux récompenses futures et peut encourager l'agent à se concentrer davantage sur les récompenses immédiates, tandis qu'un gamma plus élevé accorde plus de poids aux récompenses futures et encourage l'agent à planifier plus loin dans l'avenir.\n",
    "\n",
    "- **clip_range**: \n",
    "\n",
    "la plage de clip utilisée dans la fonction de perte de substitution. Cela aide à empêcher l'agent de mettre à jour la politique trop rapidement en une seule étape, ce qui peut conduire à une instabilité dans le processus d'entraînement. Trouver une plage de clip appropriée est important pour la stabilité et la performance.\n",
    "\n",
    "- **ent_coef**: \n",
    "\n",
    "le coefficient d'entropie utilisé dans la fonction de perte. Cela encourage l'exploration en pénalisant les politiques qui sont trop déterministes. Cependant, une entropie trop élevée peut conduire à de mauvaises performances si l'agent devient trop aléatoire. Trouver un équilibre approprié entre l'exploration et l'exploitation est important.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carracingenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
